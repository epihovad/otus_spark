{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Создаём SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('CrimeData') \\\n",
    "    .config(\"spark.hadoop.home.dir\", \"C:\\\\hadoop\") \\\n",
    "    .config(\"spark.hadoop.io.nativeio.useLegacyAccess\", \"true\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Проверка версии Spark\n",
    "print(f'Spark version: {spark.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+--------------------+--------------------+--------+--------------+--------+-------------------+----+-----+-----------+----+----------+-----------+-----------+------------+--------------------+\n",
      "|INCIDENT_NUMBER|OFFENSE_CODE|  OFFENSE_CODE_GROUP| OFFENSE_DESCRIPTION|DISTRICT|REPORTING_AREA|SHOOTING|   OCCURRED_ON_DATE|YEAR|MONTH|DAY_OF_WEEK|HOUR|  UCR_PART|     STREET|        Lat|        Long|            Location|\n",
      "+---------------+------------+--------------------+--------------------+--------+--------------+--------+-------------------+----+-----+-----------+----+----------+-----------+-----------+------------+--------------------+\n",
      "|     I182070945|         619|             Larceny|  LARCENY ALL OTHERS|     D14|           808|    NULL|2018-09-02 13:00:00|2018|    9|     Sunday|  13|  Part One| LINCOLN ST|42.35779134|-71.13937053|(42.35779134, -71...|\n",
      "|     I182070943|        1402|           Vandalism|           VANDALISM|     C11|           347|    NULL|2018-08-21 00:00:00|2018|    8|    Tuesday|   0|  Part Two|   HECLA ST|42.30682138|-71.06030035|(42.30682138, -71...|\n",
      "|     I182070941|        3410|               Towed| TOWED MOTOR VEHICLE|      D4|           151|    NULL|2018-09-03 19:27:00|2018|    9|     Monday|  19|Part Three|CAZENOVE ST|42.34658879|-71.07242943|(42.34658879, -71...|\n",
      "|     I182070940|        3114|Investigate Property|INVESTIGATE PROPERTY|      D4|           272|    NULL|2018-09-03 21:16:00|2018|    9|     Monday|  21|Part Three| NEWCOMB ST|42.33418175|-71.07866441|(42.33418175, -71...|\n",
      "|     I182070938|        3114|Investigate Property|INVESTIGATE PROPERTY|      B3|           421|    NULL|2018-09-03 21:05:00|2018|    9|     Monday|  21|Part Three|   DELHI ST|42.27536542|-71.09036101|(42.27536542, -71...|\n",
      "+---------------+------------+--------------------+--------------------+--------+--------------+--------+-------------------+----+-----+-----------+----+----------+-----------+-----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- INCIDENT_NUMBER: string (nullable = true)\n",
      " |-- OFFENSE_CODE: integer (nullable = true)\n",
      " |-- OFFENSE_CODE_GROUP: string (nullable = true)\n",
      " |-- OFFENSE_DESCRIPTION: string (nullable = true)\n",
      " |-- DISTRICT: string (nullable = true)\n",
      " |-- REPORTING_AREA: string (nullable = true)\n",
      " |-- SHOOTING: string (nullable = true)\n",
      " |-- OCCURRED_ON_DATE: timestamp (nullable = true)\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: string (nullable = true)\n",
      " |-- HOUR: integer (nullable = true)\n",
      " |-- UCR_PART: string (nullable = true)\n",
      " |-- STREET: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# file_path = Path(_file_).parent / 'data/crime_data.csv'\n",
    "file_path = '.\\data\\crime.csv'\n",
    "\n",
    "# Чтение файла\n",
    "crime_data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Вывод первых строк данных\n",
    "crime_data.show(5)\n",
    "\n",
    "# прочитаем схему данных\n",
    "crime_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество строк ДО удаления дубликатов: 319073\n",
      "Количество строк ПОСЛЕ удаления дубликатов: 319050\n"
     ]
    }
   ],
   "source": [
    "# Удаление дубликатов\n",
    "print(f\"Количество строк ДО удаления дубликатов: {crime_data.count()}\")\n",
    "crime_data = crime_data.dropDuplicates()\n",
    "print(f\"Количество строк ПОСЛЕ удаления дубликатов: {crime_data.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|summary|\n",
      "+-------+\n",
      "|  count|\n",
      "|   mean|\n",
      "| stddev|\n",
      "|    min|\n",
      "|    max|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Проверить наличие пропущенных значений\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "missing_data = crime_data.select([col(c).isNull().alias(c) for c in crime_data.columns])\n",
    "missing_data_summary = missing_data.describe().show()\n",
    "\n",
    "# Удалить строки с пропущенными критически важными данными\n",
    "crime_data = crime_data.dropna(subset=['DISTRICT', 'OCCURRED_ON_DATE', 'Lat', 'Long'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "offense_codes = spark.read.csv('.\\data\\offense_codes.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Убираем дубликаты\n",
    "offense_codes = offense_codes.dropDuplicates(['CODE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+\n",
      "|CODE|                NAME|          crime_type|\n",
      "+----+--------------------+--------------------+\n",
      "| 111|MURDER, NON-NEGLI...|MURDER, NON-NEGLI...|\n",
      "| 112|KILLING OF FELON ...|KILLING OF FELON ...|\n",
      "| 113|KILLING OF FELON ...|KILLING OF FELON ...|\n",
      "| 114|KILLING OF POLICE...|KILLING OF POLICE...|\n",
      "| 121|MANSLAUGHTER - VE...|        MANSLAUGHTER|\n",
      "+----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "# Добавим колонку crime_type, содержащую первую часть строки из NAME (до первого разделителя ' - '):\n",
    "offense_codes = offense_codes.withColumn('crime_type', split(offense_codes['NAME'], ' - ').getItem(0))\n",
    "offense_codes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+------------------+--------------------+--------+--------------+--------+-------------------+----+-----+-----------+----+----------+--------------------+-----------+------------+--------------------+--------------+\n",
      "|INCIDENT_NUMBER|OFFENSE_CODE|OFFENSE_CODE_GROUP| OFFENSE_DESCRIPTION|DISTRICT|REPORTING_AREA|SHOOTING|   OCCURRED_ON_DATE|YEAR|MONTH|DAY_OF_WEEK|HOUR|  UCR_PART|              STREET|        Lat|        Long|            Location|    crime_type|\n",
      "+---------------+------------+------------------+--------------------+--------+--------------+--------+-------------------+----+-----+-----------+----+----------+--------------------+-----------+------------+--------------------+--------------+\n",
      "|     I182055198|         413|Aggravated Assault|ASSAULT - AGGRAVA...|     E13|           304|       Y|2018-07-13 14:01:00|2018|    7|     Friday|  14|  Part One|       WASHINGTON ST|42.31667114|-71.09743705|(42.31667114, -71...|       ASSAULT|\n",
      "|     I182031562|        3130|   Search Warrants|      SEARCH WARRANT|      D4|           285|       Y|2018-04-27 14:00:00|2018|    4|     Friday|  14|Part Three|         COVENTRY ST|42.33695098|-71.08574813|(42.33695098, -71...|SEARCH WARRANT|\n",
      "|     I182028394|         413|Aggravated Assault|ASSAULT - AGGRAVA...|      B2|           319|       Y|2018-04-16 13:02:00|2018|    4|     Monday|  13|  Part One|       BLUE HILL AVE|42.30711484|-71.08425151|(42.30711484, -71...|       ASSAULT|\n",
      "|     I182025516|         413|Aggravated Assault|ASSAULT - AGGRAVA...|      B3|           944|       Y|2018-04-05 22:35:00|2018|    4|   Thursday|  22|  Part One|             AMES ST|42.28956988|-71.08510501|(42.28956988, -71...|       ASSAULT|\n",
      "|     I182049671|        1510|Firearm Violations|WEAPON - FIREARM ...|      B2|           329|       Y|2018-06-25 12:42:00|2018|    6|     Monday|  12|  Part Two|BISHOP JOE L SMIT...|42.30660388|-71.08055038|(42.30660388, -71...|        WEAPON|\n",
      "+---------------+------------+------------------+--------------------+--------+--------------+--------+-------------------+----+-----+-----------+----+----------+--------------------+-----------+------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Присоединяем offense_codes к crime_data\n",
    "crime_data = crime_data.join(offense_codes, crime_data['OFFENSE_CODE'] == offense_codes['CODE'], how='left')\n",
    "\n",
    "# Сохраняем только нужные колонки\n",
    "crime_data = crime_data.drop('CODE', 'NAME')\n",
    "crime_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------------+-----------+------------+\n",
      "|DISTRICT|OFFENSE_CODE|    crime_type|        Lat|        Long|\n",
      "+--------+------------+--------------+-----------+------------+\n",
      "|     E13|         413|       ASSAULT|42.31667114|-71.09743705|\n",
      "|      D4|        3130|SEARCH WARRANT|42.33695098|-71.08574813|\n",
      "|      B2|         413|       ASSAULT|42.30711484|-71.08425151|\n",
      "|      B3|         413|       ASSAULT|42.28956988|-71.08510501|\n",
      "|      B2|        1510|        WEAPON|42.30660388|-71.08055038|\n",
      "+--------+------------+--------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Просмотр первых строк обновлённого DataFrame\n",
    "crime_data.select('DISTRICT', 'OFFENSE_CODE', 'crime_type', 'Lat', 'Long').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Общее количество преступлений (crimes_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "crimes_total = crime_data.groupBy('DISTRICT').agg(count('*').alias('crimes_total'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Медиана числа преступлений в месяц (crimes_monthly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, percentile_approx\n",
    "\n",
    "# Создаём колонку year_month для уникального идентификатора месяца\n",
    "crime_data = crime_data.withColumn(\"year_month\", year(\"OCCURRED_ON_DATE\") * 100 + month(\"OCCURRED_ON_DATE\"))\n",
    "\n",
    "# Считаем количество преступлений в месяц для каждого района\n",
    "monthly_crimes = crime_data.groupBy(\"DISTRICT\", \"year_month\").agg(count(\"*\").alias(\"monthly_crimes\"))\n",
    "\n",
    "# Считаем медиану числа преступлений в месяц для каждого района\n",
    "crimes_monthly = monthly_crimes.groupBy(\"DISTRICT\").agg(\n",
    "    percentile_approx(\"monthly_crimes\", 0.5).alias(\"crimes_monthly\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Три самых частых типа преступлений (frequent_crime_types):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, collect_list, concat_ws\n",
    "\n",
    "# Считаем частоту каждого crime_type в районе\n",
    "crime_types_count = crime_data.groupBy('DISTRICT', 'crime_type').count()\n",
    "\n",
    "# Получаем топ-3 типов преступлений в районе\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "window = Window.partitionBy('DISTRICT').orderBy(col('count').desc())\n",
    "\n",
    "top_crime_types = crime_types_count.withColumn('rank', row_number().over(window)) \\\n",
    "    .filter(col('rank') <= 3) \\\n",
    "    .groupBy('DISTRICT').agg(\n",
    "        concat_ws(', ', collect_list('crime_type')).alias('frequent_crime_types')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Средние координаты района (lat и lng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "avg_coordinates = crime_data.groupBy(\"DISTRICT\").agg(\n",
    "    mean(\"Lat\").alias(\"lat\"),\n",
    "    mean(\"Long\").alias(\"lng\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение витрины\n",
    "\n",
    "### Подсчёт метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, col, expr, mean, approx_count_distinct, collect_list, lit, percentile_approx\n",
    "\n",
    "# crimes_total: общее количество преступлений в районе\n",
    "crimes_total = crime_data.groupBy('DISTRICT').agg(count('*').alias('crimes_total'))\n",
    "\n",
    "# crimes_monthly: медиана числа преступлений в месяц\n",
    "crime_data = crime_data.withColumn('year_month', expr('YEAR * 100 + MONTH'))\n",
    "crimes_monthly = crime_data.groupBy('DISTRICT', 'year_month').agg(count('*').alias('monthly_crimes'))\n",
    "crimes_median = crimes_monthly.groupBy('DISTRICT').agg(\n",
    "    percentile_approx('monthly_crimes', 0.5).alias('crimes_monthly')\n",
    ")\n",
    "\n",
    "# frequent_crime_types: три самых частых типа преступлений\n",
    "frequent_crime_types = crime_data.groupBy(\"DISTRICT\", \"crime_type\").count() \\\n",
    "    .groupBy(\"DISTRICT\").agg(\n",
    "        concat_ws(\", \", collect_list(\"crime_type\")).alias(\"frequent_crime_types\")\n",
    "    )\n",
    "\n",
    "# lat, lng: средние координаты\n",
    "avg_coordinates = crime_data.groupBy('DISTRICT').agg(\n",
    "    mean('Lat').alias('lat'),\n",
    "    mean('Long').alias('lng')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Объединение всех метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+----------+--------------+--------------------+------------------+------------------+\n",
      "|DISTRICT|crimes_total|year_month|monthly_crimes|frequent_crime_types|               lat|               lng|\n",
      "+--------+------------+----------+--------------+--------------------+------------------+------------------+\n",
      "|      C6|       21752|    201702|           435|DRUGS, SICK/INJUR...|42.212105037157954|-70.85558197403466|\n",
      "|      C6|       21752|    201803|           528|DRUGS, SICK/INJUR...|42.212105037157954|-70.85558197403466|\n",
      "|      C6|       21752|    201711|           446|DRUGS, SICK/INJUR...|42.212105037157954|-70.85558197403466|\n",
      "|      C6|       21752|    201603|           642|DRUGS, SICK/INJUR...|42.212105037157954|-70.85558197403466|\n",
      "|      C6|       21752|    201808|           703|DRUGS, SICK/INJUR...|42.212105037157954|-70.85558197403466|\n",
      "|      C6|       21752|    201607|           572|DRUGS, SICK/INJUR...|42.212105037157954|-70.85558197403466|\n",
      "|      C6|       21752|    201509|           575|DRUGS, SICK/INJUR...|42.212105037157954|-70.85558197403466|\n",
      "|      C6|       21752|    201611|           515|DRUGS, SICK/INJUR...|42.212105037157954|-70.85558197403466|\n",
      "|      C6|       21752|    201703|           571|DRUGS, SICK/INJUR...|42.212105037157954|-70.85558197403466|\n",
      "|      C6|       21752|    201510|           579|DRUGS, SICK/INJUR...|42.212105037157954|-70.85558197403466|\n",
      "|      C6|       21752|    201706|           616|DRUGS, SICK/INJUR...|42.212105037157954|-70.85558197403466|\n",
      "|      C6|       21752|    201712|           485|DRUGS, SICK/INJUR...|42.212105037157954|-70.85558197403466|\n",
      "|      C6|       21752|    201506|           295|DRUGS, SICK/INJUR...|42.212105037157954|-70.85558197403466|\n",
      "|      C6|       21752|    201707|           564|DRUGS, SICK/INJUR...|42.212105037157954|-70.85558197403466|\n",
      "|      C6|       21752|    201511|           517|DRUGS, SICK/INJUR...|42.212105037157954|-70.85558197403466|\n",
      "|      C6|       21752|    201601|           556|DRUGS, SICK/INJUR...|42.212105037157954|-70.85558197403466|\n",
      "|      C6|       21752|    201610|           593|DRUGS, SICK/INJUR...|42.212105037157954|-70.85558197403466|\n",
      "|      C6|       21752|    201705|           535|DRUGS, SICK/INJUR...|42.212105037157954|-70.85558197403466|\n",
      "|      C6|       21752|    201807|           616|DRUGS, SICK/INJUR...|42.212105037157954|-70.85558197403466|\n",
      "|      C6|       21752|    201709|           636|DRUGS, SICK/INJUR...|42.212105037157954|-70.85558197403466|\n",
      "+--------+------------+----------+--------------+--------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "metrics = [crimes_total, crimes_monthly, top_crime_types, avg_coordinates]\n",
    "\n",
    "# Объединяем все метрики по колонке 'DISTRICT'\n",
    "final_aggregated_data = reduce(\n",
    "    lambda df1, df2: df1.join(df2, on='DISTRICT', how='left'),\n",
    "    metrics\n",
    ")\n",
    "\n",
    "final_aggregated_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохранение в формате .parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_path = os.path.abspath('./data/aggregated_data')\n",
    "final_aggregated_data.write.mode('overwrite').parquet(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
